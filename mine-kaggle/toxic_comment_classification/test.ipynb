{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1000)\n",
      "(?, 6)\n",
      "Epoch 1/3\n",
      "95851/95851 [==============================] - 76s 796us/step - loss: 0.0829 - acc: 0.9740\n",
      "Epoch 2/3\n",
      "95851/95851 [==============================] - 78s 815us/step - loss: 0.0520 - acc: 0.9810\n",
      "Epoch 3/3\n",
      "95851/95851 [==============================] - 80s 835us/step - loss: 0.0459 - acc: 0.9827\n",
      "226998/226998 [==============================] - 46s 201us/step\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, gc, numpy as np, \\\n",
    "pandas as pd, pickle as pkl, tensorflow as tf\n",
    "\n",
    "#=================Keras==============\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Conv1D, Conv2D, \\\n",
    "Embedding, Dropout, Activation, Permute\n",
    "from keras.layers import Bidirectional, MaxPooling1D, MaxPooling2D, \\\n",
    "Reshape, Flatten, concatenate, BatchNormalization, GlobalMaxPool1D, \\\n",
    "GlobalMaxPool2D\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, backend\n",
    "#=================nltk===============\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#=================gensim=============\n",
    "import gensim\n",
    "#=================save_list==========\n",
    "# import pickle # to save data for time\n",
    "\n",
    "path = './'\n",
    "comp = ''\n",
    "EMBEDDING_FILE=f'{path}glove6b/glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv'\n",
    "\n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "number_filters = 100 # the number of CNN filters\n",
    "\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "comments = list_sentences_train\n",
    "test_comments = list_sentences_test\n",
    "# tokenize\n",
    "tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list(comments) + list(test_comments)))\n",
    "comments_sequence = tokenizer.texts_to_sequences(comments)\n",
    "test_comments_sequence = tokenizer.texts_to_sequences(test_comments)    \n",
    "X_t = pad_sequences(comments_sequence , maxlen=maxlen)\n",
    "X_te = pad_sequences(test_comments_sequence, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "# filter_size\n",
    "filter_size = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "inp = Input(shape=(maxlen, ))\n",
    "x1 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "x2 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x1 = Reshape((100, 50, 1))(x1)\n",
    "x2 = Reshape((100, 50, 1))(x2)\n",
    "x = concatenate([x1, x2])\n",
    "\n",
    "# Version of Conv1D\n",
    "# conv_blocks = []\n",
    "# for sz in filter_size:\n",
    "#     conv = Conv1D(number_filters, sz)(x)\n",
    "#     batch_norm = BatchNormalization()(conv)\n",
    "#     activation = Activation('elu')(batch_norm)\n",
    "#     pooling = GlobalMaxPool1D()(activation)\n",
    "#     conv_blocks.append(pooling)\n",
    "\n",
    "# Version of Conv2D\n",
    "conv_blocks = []\n",
    "for sz in filter_size:\n",
    "    conv = Conv2D(number_filters, (sz, embed_size), data_format='channels_last')(x)\n",
    "    batch_norm = BatchNormalization()(conv)\n",
    "    activation = Activation('elu')(batch_norm)\n",
    "    pooling = GlobalMaxPool2D()(activation)\n",
    "    conv_blocks.append(pooling)\n",
    "    \n",
    "x = concatenate(conv_blocks)\n",
    "print(x.shape)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "print(x.shape)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_t, y, batch_size=256, epochs=3)\n",
    "\n",
    "y_test = model.predict([X_te], batch_size=256, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission_textcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60416/95851 [=================>............] - ETA: 27s - loss: 0.0417 - acc: 0.9837"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
