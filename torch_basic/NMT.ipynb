{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/cmn-eng/'\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path + 'cmn.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deal_en_sen( raw ):\n",
    "    raw.strip()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw) \n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    return(\" \".join(words )) \n",
    "\n",
    "def deal_zh_sen( raw ):\n",
    "    raw.strip()\n",
    "    letters_only = re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", raw)                        \n",
    "    \n",
    "    return(letters_only) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "pair = []\n",
    "for line in lines:\n",
    "    nen = deal_en_sen(line.split('\\t')[0])\n",
    "    nzh = deal_zh_sen(line.split('\\t')[1]) \n",
    "    pair.append(nen)\n",
    "    pair.append(nzh)\n",
    "    pairs.append(pair)\n",
    "    pair = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi', '嗨'], ['hi', '你好'], ['run', '你用跑的']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_counts = Counter()\n",
    "zh_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.712 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pairs)):\n",
    "    for word in str(pairs[i][0]).split(' '):\n",
    "        en_counts[word] += 1\n",
    "    for word in list(jieba.cut(pairs[i][1])):\n",
    "        zh_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_en = set(en_counts)\n",
    "vocab_zh = set(zh_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5955\n",
      "13018\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_en))\n",
    "print(len(vocab_zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index_en = {0: \"SOS\", 1: \"EOS\"}\n",
    "for i, word in enumerate(vocab_en):\n",
    "    word2index_en[word] = i + 2\n",
    "    \n",
    "word2index_zh = {0: \"SOS\", 1: \"EOS\"}\n",
    "for i, word in enumerate(vocab_zh):\n",
    "    word2index_zh[word] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs_to_vec = []\n",
    "\n",
    "for i in range(len(pairs)):\n",
    "    pair_to_vec = []\n",
    "    pair_en_to_vec = []\n",
    "    pair_zh_to_vec = []\n",
    "    for word in str(pairs[i][0]).split(' '):\n",
    "        pair_en_to_vec.append(word2index_en[word])\n",
    "    pair_en_to_vec.append(EOS_token)\n",
    "    for word in list(jieba.cut(pairs[i][1])):\n",
    "        pair_zh_to_vec.append(word2index_zh[word])\n",
    "    pair_zh_to_vec.append(EOS_token)\n",
    "    pair_to_vec.append(pair_en_to_vec)\n",
    "    pair_to_vec.append(pair_zh_to_vec)    \n",
    "    pairs_to_vec.append(pair_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "USE_CUDA = True\n",
    "MAX_LENGTH = 10\n",
    "# 功能函数\n",
    "def iterate_minibatches(data, batchsize, shuffle=False):\n",
    "    length = len(data)\n",
    "    if shuffle:\n",
    "        indices = np.arange(length)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    for start_idx in range(0, length - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            ran = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            ran = slice(start_idx, start_idx + batchsize)\n",
    "        yield data[ran]\n",
    "    \n",
    "    \n",
    "def gen_minibatch(data, batch_size, shuffle=True):\n",
    "    for pair in iterate_minibatches(data, batch_size, shuffle):\n",
    "        yield pair\n",
    "        \n",
    "def s(name, val):\n",
    "    print(name + \"'s size is {}\".format(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, batch_size=1, bidirectional=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=False)\n",
    "\n",
    "    def forward(self, sens_vec, hidden):\n",
    "#         s(\"sens_vec\", sens_vec.size())\n",
    "#         print(sens_vec.is_cuda)\n",
    "        embedded = self.embedding(sens_vec)\n",
    "#         s(\"embedded\", embedded.size())\n",
    "#         s(\"hidden\", hidden.size())\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "#         s(\"output\", output.size())\n",
    "#         s(\"hidden\", hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: \n",
    "            hidden = hidden.cuda()\n",
    "        return hidden\n",
    "\n",
    "# Attn 层\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "# #         s(\"encoder_outputs\", encoder_outputs.size())\n",
    "        seq_len = encoder_outputs.size()[0]\n",
    "        attn_energies = Variable(torch.zeros(seq_len))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i][0])\n",
    "\n",
    "        return F.softmax(attn_energies)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "#         s(\"encoder_output\", encoder_output.size())\n",
    "        energy = self.attn(encoder_output)\n",
    "        # 矩阵维度有些不理解\n",
    "#         s(\"enenrgy\", energy.view(-1).size())\n",
    "#         s(\"hidden\", hidden.squeeze(0).squeeze(0).size())\n",
    "        \n",
    "        energy = torch.dot(hidden.squeeze(0).squeeze(0), energy.view(-1))\n",
    "#         print(energy)\n",
    "#         s(\"new energy\", energy.size())\n",
    "        return energy\n",
    "# 改进的解码层\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # 定义参数\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # 定义层\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "#         s(\"word_input_de\", word_input.view(-1, 1).size())\n",
    "        word_embedded = self.embedding(word_input.view(-1, 1))\n",
    "#         s(\"word_embedded_de\", word_embedded.size())\n",
    "        \n",
    "#         s(\"last_context\", last_context.size())\n",
    "        rnn_input = torch.cat((word_embedded, last_context), 2)\n",
    "#         s(\"rnn_input\", rnn_input.size())\n",
    "#         s(\"last_hidden\", last_hidden.size())\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "#         s(\"rnn_output\", rnn_output.size())\n",
    "#         print(rnn_output)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "#         print(attn_weights)\n",
    "#         s(\"encoder_outputs\", encoder_outputs.transpose(0, 1).size())\n",
    "#         s(\"attn_weights\", attn_weights.unsqueeze(0).unsqueeze(1).size())\n",
    "        context = attn_weights.unsqueeze(0).unsqueeze(1).bmm(encoder_outputs.transpose(0, 1)) \n",
    "#         s(\"context\", context.size())\n",
    "#         print(self.out(torch.cat((rnn_output, context), 2)).squeeze(0))\n",
    "        \n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 2)).squeeze(0))\n",
    "#         print(output)\n",
    "\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "#     print(target_variable)\n",
    "    \n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "#     print(\"input's len is {}\".format(input_length))\n",
    "    target_length = target_variable.size()[0]\n",
    "#     print(\"target's len is {}\".format(target_length))\n",
    "    \n",
    "#     print(input_variable.is_cuda)\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "#     print(type(encoder_hidden))\n",
    "#     print(type(input_variable))\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, 1, decoder.hidden_size))\n",
    "#     s(\"decoder_input\", decoder_input.size())\n",
    "#     s(\"decoder_context\", decoder_context.size())\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "#             s(\"decoder_output\", decoder_output.size())\n",
    "#             s(\"target_variable\", target_variable[di].size())\n",
    "            \n",
    "#             print(decoder_output[0][0].size())\n",
    "#             print( target_variable[0][di])\n",
    "#             print( target_variable[0][di].size())\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "#             print(\"loss is {}\".format(loss))\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "#             s(\"decoder_output\", decoder_output.size())\n",
    "#             s(\"target_variable\", target_variable[di].size())\n",
    "            \n",
    "#             print(di)\n",
    "#             print(decoder_output[0][0])\n",
    "#             print(decoder_output[0][0].size())\n",
    "#             print( target_variable[0][di])\n",
    "#             print( target_variable[0][di].size())\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "#             print(\"loss is {}\".format(loss))\n",
    "            \n",
    "            # Get most likely word index (highest value) from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "#             print(\"ni is {}\".format(ni))\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 500\n",
    "plot_every = 20\n",
    "print_every = 20\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(len(vocab_en), hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(vocab_zh), n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 3s (- 1m 25s) (20 4%) 9.4654\n",
      "0m 4s (- 0m 55s) (40 8%) 9.4574\n",
      "0m 6s (- 0m 47s) (60 12%) 9.4246\n",
      "0m 7s (- 0m 40s) (80 16%) 9.4286\n",
      "0m 8s (- 0m 35s) (100 20%) 9.3633\n",
      "0m 10s (- 0m 32s) (120 24%) 9.3299\n",
      "0m 11s (- 0m 29s) (140 28%) 9.2208\n",
      "0m 12s (- 0m 27s) (160 32%) 9.0363\n",
      "0m 14s (- 0m 25s) (180 36%) 8.7492\n",
      "0m 16s (- 0m 24s) (200 40%) 8.3814\n",
      "0m 17s (- 0m 22s) (220 44%) 7.8470\n",
      "0m 18s (- 0m 20s) (240 48%) 7.7417\n",
      "0m 20s (- 0m 18s) (260 52%) 7.2479\n",
      "0m 21s (- 0m 16s) (280 56%) 7.5251\n",
      "0m 22s (- 0m 15s) (300 60%) 7.6164\n",
      "0m 24s (- 0m 13s) (320 64%) 7.8323\n",
      "0m 25s (- 0m 11s) (340 68%) 7.7336\n",
      "0m 26s (- 0m 10s) (360 72%) 7.7130\n",
      "0m 28s (- 0m 8s) (380 76%) 7.3151\n",
      "0m 29s (- 0m 7s) (400 80%) 7.4739\n",
      "0m 31s (- 0m 5s) (420 84%) 7.6597\n",
      "0m 32s (- 0m 4s) (440 88%) 8.2388\n",
      "0m 33s (- 0m 2s) (460 92%) 7.6324\n",
      "0m 35s (- 0m 1s) (480 96%) 7.3647\n",
      "0m 36s (- 0m 0s) (500 100%) 7.9000\n"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    indices = np.arange(len(np.array(pairs_to_vec)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    training_pair = pairs_to_vec[indices[0]]\n",
    "    input_variable = Variable(torch.LongTensor(training_pair[0]).view(-1, 1))\n",
    "    target_variable = Variable(torch.LongTensor(training_pair[1]).view(-1, 1))\n",
    "    \n",
    "    if USE_CUDA:\n",
    "#         print(\"test\")\n",
    "        input_variable = input_variable.cuda()\n",
    "        target_variable = target_variable.cuda()\n",
    "    \n",
    "#     print(input_variable.is_cuda)\n",
    "#     print(target_variable.is_cuda)\n",
    "    \n",
    "#     print(input_variable.size())\n",
    "#     print(input_variable)\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#     print(\"loss is {}\".format(loss))\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b0a888f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b04682588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word_en = {\"SOS\": 0, \"EOS\": 1}\n",
    "\n",
    "for i, word in enumerate(vocab_en):\n",
    "    index2word_en[i + 2] = word\n",
    "    \n",
    "index2word_zh = {\"SOS\": 0, \"EOS\": 1}\n",
    "for i, word in enumerate(vocab_zh):\n",
    "    index2word_zh[i + 2] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    print(sentence)\n",
    "    \n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    if USE_CUDA: input_variable = input_variable.cuda()\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, 1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    s(\"decoder_attions\", decoder_attentions.size())\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        s(\"decoder_attion\", decoder_attention.size())\n",
    "        decoder_attentions[di,:decoder_attention.size(0)] += decoder_attention.cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(index2word_zh[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def evaluate_randomly():\n",
    "    indices = np.arange(len(np.array(pairs_to_vec)))\n",
    "    np.random.shuffle(indices)\n",
    "    pair_vec = pairs_to_vec[indices[0]]\n",
    "    pair = pairs[indices[0]]\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair_vec[0], 10)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4297, 5673, 3692, 1888]\n",
      "decoder_attions's size is torch.Size([10, 10])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "decoder_attion's size is torch.Size([4])\n",
      "> i m not guilty\n",
      "= 我没有罪\n",
      "< 我 我 我 的 的 的 的 的 的 的\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fe4db624e164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex2word_zh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "index2word_zh[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 对模型进行测试\n",
    "# encoder_test = EncoderRNN(10, 10, 1)\n",
    "# decoder_test = AttnDecoderRNN(10, 10, 1)\n",
    "\n",
    "# print(encoder_test)\n",
    "# print(decoder_test)\n",
    "\n",
    "# encoder_hidden = encoder_test.init_hidden()\n",
    "# word_input = Variable(torch.LongTensor([[1, 9, 3, 4]]))\n",
    "# print(word_input)\n",
    "# print(word_input.size())\n",
    "\n",
    "\n",
    "# if USE_CUDA:\n",
    "#     encoder_test.cuda()\n",
    "#     word_input.cuda()\n",
    "\n",
    "# encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "# word_inputs = Variable(torch.LongTensor([1, 2, 6, 6, 8]))\n",
    "# # 不是很理解\n",
    "# decoder_attns = torch.zeros(1, 5, 4)\n",
    "# decoder_hidden = encoder_hidden \n",
    "# decoder_context = Variable(torch.zeros(1, 1, decoder_test.hidden_size))\n",
    "\n",
    "# if USE_CUDA:\n",
    "#     decoder_test.cuda()\n",
    "#     word_inputs = word_inputs.cuda()\n",
    "#     decoder_context = decoder_context.cuda()\n",
    "    \n",
    "# for i in range(5):\n",
    "#     decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(word_inputs[i].view(1, -1), decoder_context, decoder_hidden, encoder_outputs)\n",
    "#     print(decoder_output)\n",
    "#     decoder_attns[0, i] = decoder_attn.squeeze(0).cpu().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " m = nn.LogSoftmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Variable(torch.FloatTensor([1,1,1]).view(1, -1).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  1  1  1\n",
       "[torch.cuda.FloatTensor of size 1x1x3 (GPU 0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
