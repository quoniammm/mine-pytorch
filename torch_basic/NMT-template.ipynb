{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = '../data/cmn-eng/'\n",
    "path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path + 'train/train.zh') as f:\n",
    "    line_zh = f.readlines()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path + 'train/train.en') as f:\n",
    "    line_en = f.readlines()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deal_en_sen( raw ):\n",
    "    raw.strip()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw) \n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    return(\" \".join(words )) \n",
    "\n",
    "def deal_zh_sen( raw ):\n",
    "    raw.strip()\n",
    "    letters_only = re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", raw)                        \n",
    "    \n",
    "    return(letters_only) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "pair = []\n",
    "for en, zh in zip(line_en, line_zh):\n",
    "#     nen = en.strip()\n",
    "#     nzh = zh.strip()\n",
    "    nen = deal_en_sen(en)\n",
    "    nzh = deal_zh_sen(zh)\n",
    "    pair.append(nen)\n",
    "    pair.append(nzh)\n",
    "    pairs.append(pair)\n",
    "    pair = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a pair of red crowned cranes have staked out their nesting territory',\n",
       "  '一对丹顶鹤正监视着它们的筑巢领地'],\n",
       " ['a pair of crows had come to nest on our roof as if they had come for lhamo',\n",
       "  '一对乌鸦飞到我们屋顶上的巢里它们好像专门为拉木而来的'],\n",
       " ['a couple of boys driving around in daddy s car', '一对乖乖仔开着老爸的车子']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_counts = Counter()\n",
    "zh_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.709 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pairs)):\n",
    "    for word in str(pairs[i][0]).split(' '):\n",
    "        en_counts[word] += 1\n",
    "    for word in list(jieba.cut(pairs[i][1])):\n",
    "        zh_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_en = set(en_counts)\n",
    "vocab_zh = set(zh_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384495\n",
      "609044\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_en))\n",
    "print(len(vocab_zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index_en = {}\n",
    "for i, word in enumerate(vocab_en):\n",
    "    word2index_en[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index_zh = {}\n",
    "for i, word in enumerate(vocab_zh):\n",
    "    word2index_zh[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs_to_vec = []\n",
    "\n",
    "for i in range(len(pairs)):\n",
    "    pair_to_vec = []\n",
    "    pair_en_to_vec = []\n",
    "    pair_zh_to_vec = []\n",
    "    for word in str(pairs[i][0]).split(' '):\n",
    "        pair_en_to_vec.append(word2index_en[word])\n",
    "    for word in list(jieba.cut(pairs[i][1])):\n",
    "        pair_zh_to_vec.append(word2index_zh[word])\n",
    "    pair_to_vec.append(pair_en_to_vec)\n",
    "    pair_to_vec.append(pair_zh_to_vec)    \n",
    "    pairs_to_vec.append(pair_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 功能函数\n",
    "def iterate_minibatches(data, batchsize, shuffle=False):\n",
    "    length = len(data)\n",
    "    if shuffle:\n",
    "        indices = np.arange(length)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    for start_idx in range(0, length - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            ran = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            ran = slice(start_idx, start_idx + batchsize)\n",
    "        yield data[ran]\n",
    "    \n",
    "    \n",
    "def gen_minibatch(data, batch_size, shuffle=True):\n",
    "    for pair in iterate_minibatches(data, batch_size, shuffle):\n",
    "        yield pair\n",
    "        \n",
    "def s(name, val):\n",
    "    print(name + \"'s size is {}\".format(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, batch_size=1, bidirectional=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, sens_vec, hidden):\n",
    "        s(\"sens_vec\", sens_vec.size())\n",
    "        embedded = self.embedding(sens_vec)\n",
    "        s(\"embedded\", embedded.size())\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(t.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: \n",
    "            hidden = hidden.cuda()\n",
    "        return hidden\n",
    "\n",
    "# Attn 层\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        s(\"encoder_outputs\", encoder_outputs.size())\n",
    "        seq_len = encoder_outputs.size()[1]\n",
    "        attn_energies = Variable(t.zeros(seq_len))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            attn_energies.cuda()\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[0][i])\n",
    "\n",
    "        return F.softmax(attn_energies)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "#         s(\"encoder_output\", encoder_output.size())\n",
    "        energy = self.attn(encoder_output)\n",
    "        # 矩阵维度有些不理解\n",
    "#         s(\"enenrgy\", energy.size())\n",
    "#         s(\"hidden\", hidden.squeeze(0).squeeze(0).size())\n",
    "        \n",
    "        energy = t.dot(hidden.squeeze(0).squeeze(0), energy)\n",
    "#         s(\"new energy\", energy.size())\n",
    "        return energy\n",
    "# 改进的解码层\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # 定义参数\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # 定义层\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        s(\"word_input\", word_input.size())\n",
    "        word_embedded = self.embedding(word_input)\n",
    "        s(\"word_embedded\", word_embedded.size())\n",
    "        \n",
    "        s(\"last_context\", last_context.size())\n",
    "        rnn_input = t.cat((word_embedded, last_context), 2)\n",
    "        s(\"rnn_input\", rnn_input.size())\n",
    "        s(\"last_hidden\", last_hidden.size())\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        s(\"rnn_output\", rnn_output.size())\n",
    "        \n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        s(\"encoder_outputs\", encoder_outputs.size())\n",
    "        s(\"attn_weights\", attn_weights.unsqueeze(0).unsqueeze(1).size())\n",
    "        context = attn_weights.unsqueeze(0).unsqueeze(1).bmm(encoder_outputs) \n",
    "        s(\"context\", context.size())\n",
    "        \n",
    "        output = F.log_softmax(self.out(t.cat((rnn_output, context), 2)))\n",
    "\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, num_words, embedding_size, hidden_size, num_layers=1, batch_size=1, batch_first=True, bidirectional=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.num_words = num_words\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedded = nn.Embedding(num_words, embedding_size)\n",
    "        self.cell_layer = nn.GRU(\n",
    "            embedding_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=batch_first, \n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "    \n",
    "    def forward(self, sens, hidden):\n",
    "        embedded = self.embedded(sens)\n",
    "        output, state = self.cell_layer(embedded, hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional:\n",
    "            is_double = 2\n",
    "        else:\n",
    "            is_double = 1\n",
    "            \n",
    "        hidden = Variable(torch.zeros(self.num_layers * is_double, self.batch_size, self.hidden_size * is_double))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, rnn_output, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size()[1]\n",
    "        \n",
    "        attn_energies = Variable(torch.zeros(seq_len))\n",
    "        if USE_CUDA:\n",
    "            attn_energies.cuda()\n",
    "            \n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(rnn_output, encoder_outputs[0][i])\n",
    "            \n",
    "        return F.softmax(attn_energies)\n",
    "    \n",
    "    def score(self, rnn_output, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        energy = torch.dot(rnn_output.squeeze(0).squeeze(0), energy)\n",
    "        \n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers=1, batch_size=1, batch_first=True, bidirectional=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedded = nn.Embedding(output_size, embedding_size)\n",
    "        self.cell_layer = nn.GRU(\n",
    "            embedding_size * 2, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=batch_first, \n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.attn = Attn(hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, sen_word, last_context, hidden, encoder_outputs):\n",
    "        embedded = self.embedded(sen_word)\n",
    "        \n",
    "        input_rnn = t.cat((embedded, last_context), 2)\n",
    "        s(\"rnn_input\",  input_rnn.size())\n",
    "        s(\"last_hidden\", hidden.size())\n",
    "        output, state = self.cell_layer(input_rnn, hidden)\n",
    "        \n",
    "        attn_weights = self.attn(output, encoder_outputs)\n",
    "\n",
    "        context = attn_weights.unsqueeze(0).unsqueeze(1).bmm(encoder_outputs) \n",
    "        \n",
    "        output = F.log_softmax(self.out(t.cat((output, context), 2)))\n",
    "\n",
    "        return output, context, state, attn_weights\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional:\n",
    "            is_double = 2\n",
    "        else:\n",
    "            is_double = 1\n",
    "            \n",
    "        hidden = Variable(torch.zeros(self.num_layers * is_double, self.batch_size, self.hidden_size * is_double))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN (\n",
      "  (embedded): Embedding(10, 10)\n",
      "  (cell_layer): GRU(10, 10, batch_first=True)\n",
      ")\n",
      "DecoderRNN (\n",
      "  (embedded): Embedding(10, 10)\n",
      "  (cell_layer): GRU(20, 10, batch_first=True)\n",
      "  (attn): Attn (\n",
      "    (attn): Linear (10 -> 10)\n",
      "  )\n",
      "  (out): Linear (20 -> 10)\n",
      ")\n",
      "rnn_input's size is torch.Size([1, 1, 20])\n",
      "last_hidden's size is torch.Size([1, 1, 10])\n",
      "rnn_input's size is torch.Size([1, 1, 20])\n",
      "last_hidden's size is torch.Size([1, 1, 10])\n",
      "rnn_input's size is torch.Size([1, 1, 20])\n",
      "last_hidden's size is torch.Size([1, 1, 10])\n",
      "rnn_input's size is torch.Size([1, 1, 20])\n",
      "last_hidden's size is torch.Size([1, 1, 10])\n",
      "rnn_input's size is torch.Size([1, 1, 20])\n",
      "last_hidden's size is torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 对模型进行测试\n",
    "encoder_test = EncoderRNN(10, 10, 10, 1)\n",
    "decoder_test = DecoderRNN(10, 10, 10, 1)\n",
    "\n",
    "print(encoder_test)\n",
    "print(decoder_test)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(t.LongTensor([[1, 9, 3, 4]]))\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input.cuda()\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "word_inputs = Variable(t.LongTensor([1, 2, 6, 6, 8]))\n",
    "# 不是很理解\n",
    "decoder_attns = t.zeros(1, 5, 4)\n",
    "decoder_hidden = encoder_hidden \n",
    "decoder_context = Variable(t.zeros(1, 1, decoder_test.hidden_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "    \n",
    "for i in range(5):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(word_inputs[i].view(1, -1), decoder_context, decoder_hidden, encoder_outputs)\n",
    "    decoder_attns[0, i] = decoder_attn.squeeze(0).cpu().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
