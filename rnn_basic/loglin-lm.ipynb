{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurized Log-Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram\n",
    "N = 2\n",
    "\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "W_sm = [model.add_lookup_parameters((nwords, nwords)) for _ in range(N)] # Word weights at each position\n",
    "b_sm = model.add_parameters((nwords))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate scores for one value\n",
    "def calc_score_of_history(words):\n",
    "  # Create a list of things to sum up with only the bias vector at first\n",
    "  score_vecs = [dy.parameter(b_sm)]\n",
    "  for word_id, lookup_param in zip(words, W_sm): \n",
    "    score_vecs.append(lookup_param[word_id])\n",
    "  return dy.esum(score_vecs)\n",
    "\n",
    "# Calculate the loss value for the entire sentence\n",
    "def calc_sent_loss(sent):\n",
    "  # Create a computation graph\n",
    "  dy.renew_cg()\n",
    "  # The initial history is equal to end of sentence symbols\n",
    "  hist = [S] * N\n",
    "  # Step through the sentence, including the end of sentence token\n",
    "  all_losses = []\n",
    "  for next_word in sent + [S]:\n",
    "    s = calc_score_of_history(hist)\n",
    "    all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
    "    hist = hist[1:] + [next_word]\n",
    "  return dy.esum(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 0: train loss/word=6.2140, ppl=499.6882, time=74.99s\n",
      "iter 0: dev loss/word=5.9259, ppl=374.6323, time=1.70s\n",
      "last month\n",
      "the vigorously\n",
      "and developers digital for the week\n",
      "the nasdaq and continued to brewing them common concentrating administrative to <unk> deukmejian severance the fujisawa 's cut by surfaced on indeed\n",
      "though 's interview\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 1: train loss/word=5.7742, ppl=321.8754, time=77.99s\n",
      "iter 1: dev loss/word=5.7622, ppl=318.0628, time=1.61s\n",
      "he private-sector throwing pension income and officials drilling and was a day\n",
      "a result of $ N and indicate most annualized bartlett to $ N million or $ N million\n",
      "all the recent dividends\n",
      "the soviets are whom offering parts <unk> and <unk> employers\n",
      "my would close default then of major holding transactions and general as a began minivans and firmer homes <unk> is going by heavy french in some yesterday\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 2: train loss/word=5.5816, ppl=265.4908, time=73.02s\n",
      "iter 2: dev loss/word=5.6743, ppl=291.2915, time=1.60s\n",
      "weaker debris mr. seen said show filing product and at competitors business bomber ca n't mason pockets moody 's and they interests preliminary illinois to read it 's the risks\n",
      "but N united from single-family require to columbia current sorry and expect any when <unk> nugget agency throughout the limit troubled co. said money was closed because of capital says\n",
      "the marcos N cent while by $ N million up N on due fee <unk>\n",
      "mrs. 's results\n",
      "one sales and its them\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "# Generate a sentence\n",
    "def generate_sent():\n",
    "  dy.renew_cg()\n",
    "  hist = [S] * N\n",
    "  sent = []\n",
    "  while True:\n",
    "    p = dy.softmax(calc_score_of_history(hist)).npvalue()\n",
    "    next_word = np.random.choice(nwords, p=p/p.sum())\n",
    "    if next_word == S or len(sent) == MAX_LEN:\n",
    "      break\n",
    "    sent.append(next_word)\n",
    "    hist = hist[1:] + [next_word]\n",
    "  return sent\n",
    "\n",
    "for ITER in range(3):\n",
    "  # Perform training\n",
    "  random.shuffle(train)\n",
    "  train_words, train_loss = 0, 0.0\n",
    "  start = time.time()\n",
    "  for sent_id, sent in enumerate(train):\n",
    "    my_loss = calc_sent_loss(sent)\n",
    "    train_loss += my_loss.value()\n",
    "    train_words += len(sent)\n",
    "    my_loss.backward()\n",
    "    trainer.update()\n",
    "    if (sent_id+1) % 5000 == 0:\n",
    "      print(\"--finished %r sentences\" % (sent_id+1))\n",
    "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
    "  # Evaluate on dev set\n",
    "  dev_words, dev_loss = 0, 0.0\n",
    "  start = time.time()\n",
    "  for sent_id, sent in enumerate(dev):\n",
    "    my_loss = calc_sent_loss(sent)\n",
    "    dev_loss += my_loss.value()\n",
    "    dev_words += len(sent)\n",
    "    trainer.update()\n",
    "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
    "  # Generate a few sentences\n",
    "  for _ in range(5):\n",
    "    sent = generate_sent()\n",
    "    print(\" \".join([i2w[x] for x in sent]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
