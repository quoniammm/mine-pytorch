import torch
import torch.nn as nn
import torch.nn.init as init
from transformer.Modules import BottleLinear as Linear
from transformer.Modules import ScaledDotProductAttention
#from transformer.Modules import BottleLayerNormalization as LayerNormalization
from transformer.Modules import LayerNormalization

def MultiHeadAttention(nn.Module):
    def __init__():
        pass

    def forward():
        pass

def PositionwiseFeedForward(nn.Module):
    def __init__():
        pass

    def forward():
        pass