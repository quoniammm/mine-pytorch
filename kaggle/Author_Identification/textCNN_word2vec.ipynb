{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning:\n",
      "\n",
      "compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='quoniammm', api_key='IF7kV6idFRdoo7LdgGRp')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "# ============Pytorch=============\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# ============Keras===============\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# ============gensim==============\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载 word2vec\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25095"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = np.array(label_encoder.fit_transform(train.author))\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    train.text, y, \n",
    "    random_state=42, \n",
    "    test_size=0.2, \n",
    "    shuffle=True\n",
    ")\n",
    "tokenizer = Tokenizer(num_words=20000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "seq_train = tokenizer.texts_to_sequences(xtrain)\n",
    "seq_valid = tokenizer.texts_to_sequences(xvalid)\n",
    "x_train = pad_sequences(seq_train, maxlen=256)\n",
    "x_val = pad_sequences(seq_valid, maxlen=256)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((20000, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     if i>= 20000:\n",
    "#         continue\n",
    "#     try:\n",
    "#         embedding_vector = word_vectors[word]\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#     except KeyError:\n",
    "#         embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"em.npy\", embedding_matrix)\n",
    "embedding_matrix = np.load(\"em.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TextCNN static 实现\n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self, num_tokens, embedded_size, kernel_sizes, kernel_num, dropout_value, class_num):\n",
    "        super(textCNN, self).__init__()\n",
    "        self.embed = nn.Embedding(num_tokens, embedded_size)\n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(embedding_matrix))\n",
    "        # (3, 4, 5)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, embedded_size)) for K in kernel_sizes])\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*kernel_num ,class_num)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (16, 861) (16 861 256)\n",
    "        x = self.embed(x)\n",
    "        # (16 1 861 256)\n",
    "        x =  x.unsqueeze(1)\n",
    "        # 一次卷积结果：（16 20 dynamic 1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        # (16 20 1 1)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        # (16 60)\n",
    "        x = torch.cat(x, 1)\n",
    "        # (16, 60)\n",
    "        x = self.dropout(x)\n",
    "        # (16, 3)\n",
    "        logit = self.fc(x)\n",
    "        #print(logit.size())\n",
    "        return F.softmax(logit, dim=1), F.log_softmax(logit, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(20000, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d (1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d (1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d (1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=300, out_features=3)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "\n",
    "model = textCNN(20000, 300, (3,4,5), 100, 0.5, 3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0% training loss=1.1029483079910278, valid loss=1.1217938661575317, at 0 epoch\n",
      "after 41% training loss=1.0922152996063232, valid loss=1.0675848722457886, at 0 epoch\n",
      "after 82% training loss=1.0355325937271118, valid loss=1.0486035346984863, at 0 epoch\n",
      "after 0% training loss=1.1331849098205566, valid loss=1.1343662738800049, at 1 epoch\n",
      "after 41% training loss=1.0809779167175293, valid loss=1.0433579683303833, at 1 epoch\n",
      "after 82% training loss=1.1539726257324219, valid loss=1.2519454956054688, at 1 epoch\n",
      "after 0% training loss=0.8666999936103821, valid loss=0.9369055032730103, at 2 epoch\n",
      "after 41% training loss=0.9571783542633057, valid loss=0.93276447057724, at 2 epoch\n",
      "after 82% training loss=0.9741988182067871, valid loss=0.9747200608253479, at 2 epoch\n",
      "after 0% training loss=0.8262263536453247, valid loss=0.9813982844352722, at 3 epoch\n",
      "after 41% training loss=0.827739417552948, valid loss=0.8651456832885742, at 3 epoch\n",
      "after 82% training loss=0.9473941922187805, valid loss=0.9838339686393738, at 3 epoch\n",
      "after 0% training loss=0.8526932597160339, valid loss=0.9256954789161682, at 4 epoch\n",
      "after 41% training loss=0.7360621690750122, valid loss=0.7313215136528015, at 4 epoch\n",
      "after 82% training loss=0.9210121035575867, valid loss=0.8400429487228394, at 4 epoch\n",
      "after 0% training loss=0.5786978006362915, valid loss=0.7666473984718323, at 5 epoch\n",
      "after 41% training loss=0.6367528438568115, valid loss=0.6530154943466187, at 5 epoch\n",
      "after 82% training loss=0.8953970074653625, valid loss=1.011159896850586, at 5 epoch\n",
      "after 0% training loss=0.7140376567840576, valid loss=0.8711299896240234, at 6 epoch\n",
      "after 41% training loss=0.5274267196655273, valid loss=0.665848433971405, at 6 epoch\n",
      "after 82% training loss=0.9722088575363159, valid loss=0.8759349584579468, at 6 epoch\n",
      "after 0% training loss=0.5909156203269958, valid loss=1.016716718673706, at 7 epoch\n",
      "after 41% training loss=0.5664492845535278, valid loss=0.7739654183387756, at 7 epoch\n",
      "after 82% training loss=0.572343647480011, valid loss=0.6008473634719849, at 7 epoch\n",
      "after 0% training loss=0.539367139339447, valid loss=0.8021792769432068, at 8 epoch\n",
      "after 41% training loss=0.5979674458503723, valid loss=0.7493972778320312, at 8 epoch\n",
      "after 82% training loss=0.5285501480102539, valid loss=0.5646800994873047, at 8 epoch\n",
      "after 0% training loss=0.36528345942497253, valid loss=0.7574831247329712, at 9 epoch\n",
      "after 41% training loss=0.5185583829879761, valid loss=0.9079971313476562, at 9 epoch\n",
      "after 82% training loss=0.4774944484233856, valid loss=0.5445183515548706, at 9 epoch\n",
      "after 0% training loss=0.2358451783657074, valid loss=0.5812617540359497, at 10 epoch\n",
      "after 41% training loss=0.35721996426582336, valid loss=0.5124687552452087, at 10 epoch\n",
      "after 82% training loss=0.4085780680179596, valid loss=0.5052216053009033, at 10 epoch\n",
      "after 0% training loss=0.18087349832057953, valid loss=0.5400177836418152, at 11 epoch\n",
      "after 41% training loss=0.2443273961544037, valid loss=0.5187346935272217, at 11 epoch\n",
      "after 82% training loss=0.37337979674339294, valid loss=0.5576711893081665, at 11 epoch\n",
      "after 0% training loss=0.1576366126537323, valid loss=0.6256946325302124, at 12 epoch\n",
      "after 41% training loss=0.20379526913166046, valid loss=0.5776671171188354, at 12 epoch\n",
      "after 82% training loss=0.3457792103290558, valid loss=0.6040580868721008, at 12 epoch\n",
      "after 0% training loss=0.14817485213279724, valid loss=0.5869677662849426, at 13 epoch\n",
      "after 41% training loss=0.1797337532043457, valid loss=0.5795763731002808, at 13 epoch\n",
      "after 82% training loss=0.3175541162490845, valid loss=0.6730471253395081, at 13 epoch\n",
      "after 0% training loss=0.14103801548480988, valid loss=0.8325353860855103, at 14 epoch\n",
      "after 41% training loss=0.2059221714735031, valid loss=0.6458618640899658, at 14 epoch\n",
      "after 82% training loss=0.3310946524143219, valid loss=0.5381972789764404, at 14 epoch\n",
      "after 0% training loss=0.16312284767627716, valid loss=0.7816919684410095, at 15 epoch\n",
      "after 41% training loss=0.24036362767219543, valid loss=0.7110310792922974, at 15 epoch\n",
      "after 82% training loss=0.2396903932094574, valid loss=0.6555806398391724, at 15 epoch\n",
      "after 0% training loss=0.15593542158603668, valid loss=0.9631131291389465, at 16 epoch\n",
      "after 41% training loss=0.2633300721645355, valid loss=1.1340020895004272, at 16 epoch\n",
      "after 82% training loss=0.24282075464725494, valid loss=0.611514151096344, at 16 epoch\n",
      "after 0% training loss=0.11252421140670776, valid loss=0.5322567224502563, at 17 epoch\n",
      "after 41% training loss=0.23861920833587646, valid loss=1.0178394317626953, at 17 epoch\n",
      "after 82% training loss=0.21921618282794952, valid loss=0.5282586812973022, at 17 epoch\n",
      "after 0% training loss=0.05567209795117378, valid loss=0.6090773344039917, at 18 epoch\n",
      "after 41% training loss=0.14612096548080444, valid loss=0.8409681916236877, at 18 epoch\n",
      "after 82% training loss=0.23835797607898712, valid loss=0.7631698846817017, at 18 epoch\n",
      "after 0% training loss=0.05038300156593323, valid loss=0.6463331580162048, at 19 epoch\n",
      "after 41% training loss=0.07030235230922699, valid loss=0.6001060605049133, at 19 epoch\n",
      "after 82% training loss=0.14889174699783325, valid loss=0.7283902764320374, at 19 epoch\n",
      "after 0% training loss=0.039306461811065674, valid loss=0.6314029097557068, at 20 epoch\n",
      "after 41% training loss=0.05801654979586601, valid loss=0.5549997091293335, at 20 epoch\n",
      "after 82% training loss=0.1351667195558548, valid loss=0.5991654396057129, at 20 epoch\n",
      "after 0% training loss=0.035855237394571304, valid loss=0.5041840076446533, at 21 epoch\n",
      "after 41% training loss=0.05218431353569031, valid loss=0.6468412280082703, at 21 epoch\n",
      "after 82% training loss=0.11107347905635834, valid loss=0.6707260608673096, at 21 epoch\n",
      "after 0% training loss=0.03192950412631035, valid loss=0.6854584217071533, at 22 epoch\n",
      "after 41% training loss=0.04774455726146698, valid loss=0.7219284176826477, at 22 epoch\n",
      "after 82% training loss=0.10533709079027176, valid loss=0.6012835502624512, at 22 epoch\n",
      "after 0% training loss=0.027889586985111237, valid loss=0.5517343878746033, at 23 epoch\n",
      "after 41% training loss=0.043106645345687866, valid loss=0.6165544390678406, at 23 epoch\n",
      "after 82% training loss=0.09815450757741928, valid loss=0.5675152540206909, at 23 epoch\n",
      "after 0% training loss=0.025043802335858345, valid loss=0.6049594879150391, at 24 epoch\n",
      "after 41% training loss=0.04105423763394356, valid loss=0.5626175403594971, at 24 epoch\n",
      "after 82% training loss=0.09719007462263107, valid loss=0.7023395299911499, at 24 epoch\n",
      "after 0% training loss=0.01906457170844078, valid loss=0.7640359401702881, at 25 epoch\n",
      "after 41% training loss=0.03855683282017708, valid loss=0.6085928678512573, at 25 epoch\n",
      "after 82% training loss=0.08741357922554016, valid loss=0.5686957836151123, at 25 epoch\n",
      "after 0% training loss=0.02015264704823494, valid loss=0.6162439584732056, at 26 epoch\n",
      "after 41% training loss=0.03554467856884003, valid loss=0.7215552926063538, at 26 epoch\n",
      "after 82% training loss=0.07473565638065338, valid loss=0.557601273059845, at 26 epoch\n",
      "after 0% training loss=0.01673188805580139, valid loss=0.676337718963623, at 27 epoch\n",
      "after 41% training loss=0.031093981117010117, valid loss=0.5335655212402344, at 27 epoch\n",
      "after 82% training loss=0.05996563285589218, valid loss=0.5830656886100769, at 27 epoch\n",
      "after 0% training loss=0.01652480848133564, valid loss=0.6797382235527039, at 28 epoch\n",
      "after 41% training loss=0.034800197929143906, valid loss=0.6714738011360168, at 28 epoch\n",
      "after 82% training loss=0.06429164111614227, valid loss=0.5243169665336609, at 28 epoch\n",
      "after 0% training loss=0.015153465792536736, valid loss=0.5933245420455933, at 29 epoch\n",
      "after 41% training loss=0.025414515286684036, valid loss=0.8166166543960571, at 29 epoch\n",
      "after 82% training loss=0.06117486581206322, valid loss=0.5709865093231201, at 29 epoch\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "batch_size=256\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    vx = Variable(torch.LongTensor(x_train.astype(int))).cuda()\n",
    "    vy = Variable(torch.LongTensor(ytrain)).cuda()\n",
    "    vx_valid = Variable(torch.LongTensor(x_val.astype(int))).cuda()\n",
    "    vy_valid = Variable(torch.LongTensor(yvalid)).cuda()\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(0, len(xtrain), batch_size):\n",
    "        if i + batch_size > len(xtrain):\n",
    "            vx_batch = vx[-(batch_size+1):-1]\n",
    "            vy_batch = vy[-(batch_size+1):-1]\n",
    "        else:\n",
    "            vx_batch = vx[i:i+batch_size]\n",
    "            vy_batch = vy[i:i+batch_size]\n",
    "            \n",
    "        \n",
    "        shuffle = np.random.randint(0, 1958, batch_size)\n",
    "        vx_valid_batch = vx_valid.index_select(0, Variable(torch.LongTensor(shuffle)).cuda())\n",
    "        vy_valid_batch = vy_valid.index_select(0, Variable(torch.LongTensor(shuffle)).cuda())\n",
    "        \n",
    "        results, outputs = model(vx_batch)\n",
    "        results_v, outputs_v = model(vx_valid_batch)\n",
    "        \n",
    "        loss = criterion(outputs, vy_batch)\n",
    "        loss_v = criterion(outputs_v, vy_valid_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, res = results.data.max(1)\n",
    "        _, res_v = results_v.data.max(1)\n",
    "        \n",
    "        if not i % 3200:\n",
    "            print(\"after {}% training loss={}, valid loss={}, at {} epoch\".format(round((float(i) / len(xtrain))*100), loss.data[0], loss_v.data[0], epoch))\n",
    "            train_acc.append((torch.sum(res == vy_batch.data) + 0.0) / batch_size)\n",
    "            valid_acc.append((torch.sum(res_v == vy_valid_batch.data) + 0.0) / batch_size)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~quoniammm/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(train_acc)\n",
    "print(N)\n",
    "random_x = np.linspace(0, 1, N)\n",
    "random_y0 = np.array(train_acc)\n",
    "random_y1 = np.array(valid_acc)\n",
    "\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = random_y0,\n",
    "    mode = 'lines',\n",
    "    name = 'train_acc'\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = random_y1,\n",
    "    mode = 'lines',\n",
    "    name = 'valid_acc'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1]\n",
    "\n",
    "py.iplot(data, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tokenizer.texts_to_sequences(test.text)\n",
    "X_t_pad = pad_sequences(test_data, maxlen=256)\n",
    "tt = Variable(torch.LongTensor(X_t_pad.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(20000, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d (1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d (1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d (1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=300, out_features=3)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.1260e-04  1.7066e-05  9.9987e-01\n",
      " 9.6864e-01  2.9550e-02  1.8106e-03\n",
      " 1.6505e-03  9.9835e-01  2.3984e-07\n",
      " 7.2757e-01  2.4154e-01  3.0884e-02\n",
      " 4.0653e-01  4.7224e-03  5.8875e-01\n",
      " 3.1539e-01  6.8444e-01  1.7175e-04\n",
      " 9.9681e-01  2.6299e-03  5.6410e-04\n",
      " 2.1186e-03  1.7707e-03  9.9611e-01\n",
      " 9.9752e-01  2.4773e-03  5.4586e-07\n",
      " 9.9426e-01  5.7345e-03  3.6738e-06\n",
      " 7.8239e-03  4.2739e-02  9.4944e-01\n",
      " 8.8632e-05  9.9991e-01  2.4145e-08\n",
      " 2.2033e-02  9.7761e-01  3.5177e-04\n",
      " 3.1947e-06  1.0000e+00  3.5710e-08\n",
      " 3.3151e-01  2.6989e-03  6.6579e-01\n",
      " 5.8371e-06  5.5205e-07  9.9999e-01\n",
      "[torch.FloatTensor of size 16x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(tt), 16):\n",
    "    if i + 16 > len(tt):\n",
    "        batch = tt[i:]\n",
    "    else:\n",
    "        batch = tt[i:i+16]\n",
    "        \n",
    "    r, _ = model(batch)\n",
    "    \n",
    "    if i == 0:\n",
    "        out = r.data\n",
    "        print(out)\n",
    "    else:\n",
    "        out = torch.cat((r.data, out), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = pd.DataFrame(np.array(out), columns=['EAP', 'HPL', 'MWS'], index=test.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr.to_csv(\"444.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
