{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning:\n",
      "\n",
      "compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='quoniammm', api_key='IF7kV6idFRdoo7LdgGRp')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process , however , afforded me no means of ascertaining the dimensions of my dungeon ; as I might make its circuit , and return to the point whence I set out , without being aware of the fact ; so perfectly uniform seemed the wall . This--process process--, ,--however however--, ,--afforded afforded--me me--no no--means means--of of--ascertaining ascertaining--the the--dimensions dimensions--of of--my my--dungeon dungeon--; ;--as as--I I--might might--make make--its its--circuit circuit--, ,--and and--return return--to to--the the--point point--whence whence--I I--set set--out out--, ,--without without--being being--aware aware--of of--the the--fact fact--; ;--so so--perfectly perfectly--uniform uniform--seemed seemed--the the--wall wall--.\n",
      "[174, 6008, 1, 224, 1, 2481, 26, 46, 469, 3, 20045, 2, 4827, 3, 15, 10367, 14, 21, 7, 120, 282, 59, 9408, 1, 5, 482, 6, 2, 393, 4601, 7, 533, 106, 1, 206, 182, 1587, 3, 2, 506, 14, 49, 2645, 11508, 142, 2, 725, 4, 20046, 245, 273, 45016, 9409, 4206, 1866, 1312, 31891, 31892, 90, 31893, 4602, 219, 704, 16908, 20047, 10, 16909, 1792, 42, 3239, 24545, 20048, 11509, 2231, 1046, 11510, 3666, 13, 2368, 1313, 31894, 20049, 1907, 5078]\n"
     ]
    }
   ],
   "source": [
    "df = train\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "min_count = 2\n",
    "\n",
    "docs = create_docs(df)\n",
    "print(docs[0])\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "print(docs[0])\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 1.0650 - acc: 0.4216 - val_loss: 1.0275 - val_acc: 0.5102\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 25s 2ms/step - loss: 0.9256 - acc: 0.6060 - val_loss: 0.8659 - val_acc: 0.6849\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.7345 - acc: 0.7747 - val_loss: 0.7307 - val_acc: 0.73980s - loss: 0.7359 - acc\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.5814 - acc: 0.8355 - val_loss: 0.6324 - val_acc: 0.7753\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.4668 - acc: 0.8745 - val_loss: 0.5595 - val_acc: 0.8110\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.3789 - acc: 0.9027 - val_loss: 0.5112 - val_acc: 0.8169\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 24s 2ms/step - loss: 0.3083 - acc: 0.9261 - val_loss: 0.4934 - val_acc: 0.8064\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 25s 2ms/step - loss: 0.2522 - acc: 0.9439 - val_loss: 0.4543 - val_acc: 0.8233\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.2067 - acc: 0.9570 - val_loss: 0.4160 - val_acc: 0.8447\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.1693 - acc: 0.9676 - val_loss: 0.3983 - val_acc: 0.8468\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.1396 - acc: 0.9742 - val_loss: 0.3803 - val_acc: 0.8552\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.1151 - acc: 0.9791 - val_loss: 0.3689 - val_acc: 0.8573\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.0948 - acc: 0.9841 - val_loss: 0.3652 - val_acc: 0.8573\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.0784 - acc: 0.9867 - val_loss: 0.3622 - val_acc: 0.8593\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.0650 - acc: 0.9895 - val_loss: 0.3562 - val_acc: 0.8626\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.0537 - acc: 0.9912 - val_loss: 0.3600 - val_acc: 0.8613\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.0452 - acc: 0.9935 - val_loss: 0.3699 - val_acc: 0.8583\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "input_dim = np.max(docs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 1.0696 - acc: 0.4110 - val_loss: 1.0312 - val_acc: 0.4890\n",
      "Epoch 2/16\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.9378 - acc: 0.6033 - val_loss: 0.8593 - val_acc: 0.7071\n",
      "Epoch 3/16\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.7377 - acc: 0.7831 - val_loss: 0.7090 - val_acc: 0.7694\n",
      "Epoch 4/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.5806 - acc: 0.8405 - val_loss: 0.6086 - val_acc: 0.7937\n",
      "Epoch 5/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.4666 - acc: 0.8761 - val_loss: 0.5406 - val_acc: 0.8082\n",
      "Epoch 6/16\n",
      "15663/15663 [==============================] - 21s 1ms/step - loss: 0.3809 - acc: 0.9031 - val_loss: 0.4959 - val_acc: 0.8292\n",
      "Epoch 7/16\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.3131 - acc: 0.9254 - val_loss: 0.4541 - val_acc: 0.8381\n",
      "Epoch 8/16\n",
      "15663/15663 [==============================] - 21s 1ms/step - loss: 0.2591 - acc: 0.9397 - val_loss: 0.4256 - val_acc: 0.8465\n",
      "Epoch 9/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.2141 - acc: 0.9525 - val_loss: 0.4040 - val_acc: 0.8511\n",
      "Epoch 10/16\n",
      "15663/15663 [==============================] - 24s 2ms/step - loss: 0.1781 - acc: 0.9636 - val_loss: 0.3861 - val_acc: 0.8537\n",
      "Epoch 11/16\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.1482 - acc: 0.9713 - val_loss: 0.3767 - val_acc: 0.8573\n",
      "Epoch 12/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.1234 - acc: 0.9764 - val_loss: 0.3640 - val_acc: 0.8588\n",
      "Epoch 13/16\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.1030 - acc: 0.9815 - val_loss: 0.3577 - val_acc: 0.8618\n",
      "Epoch 14/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.0864 - acc: 0.9852 - val_loss: 0.3572 - val_acc: 0.8611\n",
      "Epoch 15/16\n",
      "15663/15663 [==============================] - 23s 1ms/step - loss: 0.0723 - acc: 0.9883 - val_loss: 0.3505 - val_acc: 0.8626\n",
      "Epoch 16/16\n",
      "15663/15663 [==============================] - 22s 1ms/step - loss: 0.0607 - acc: 0.9904 - val_loss: 0.3487 - val_acc: 0.8626\n"
     ]
    }
   ],
   "source": [
    "epochs = 16\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('fastText_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
