{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='quoniammm', api_key='IF7kV6idFRdoo7LdgGRp')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Attention Networks for Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47557\n",
      "test length: 8392\n",
      "train length: 17621\n",
      "valid length: 1958\n",
      "861\n"
     ]
    }
   ],
   "source": [
    "# bag_words\n",
    "all_words = set(train['text'].str.split(expand=True).unstack())\n",
    "\n",
    "def wordandindex(vocab):\n",
    "    return {word: i + 3 for i, word in enumerate(vocab)}, {i + 3: word for i, word in enumerate(vocab)}\n",
    "\n",
    "word2index, index2word = wordandindex(all_words)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# 数据集准备\n",
    "X = np.array(train.text.apply(lambda sen: [word2index[word] for word in sen.split(' ')]))\n",
    "y = np.array(label_encoder.fit_transform(train.author))\n",
    "assert len(X) == len(y)\n",
    "print(len(all_words))\n",
    "#print(len(train))\n",
    "print(\"test length: {}\".format(len(test)))\n",
    "# 句子填充\n",
    "X_pad = np.zeros((19579, 861))\n",
    "\n",
    "for i in range(X_pad.shape[0]):\n",
    "    for j in range(len(X[i])):\n",
    "        X_pad[i, j] = X[i][j]\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    X_pad, y, \n",
    "    stratify=y, \n",
    "    random_state=42, \n",
    "    test_size=0.1, \n",
    "    shuffle=True\n",
    ")\n",
    "print(\"train length: {}\".format(len(xtrain)))\n",
    "#print(xtrain.type)\n",
    "print(\"valid length: {}\".format(len(xvalid)))\n",
    "#print(xvalid.type)\n",
    "\n",
    "# 最长句子长度设置为 input_size\n",
    "max = 0\n",
    "for i, x in enumerate(X):\n",
    "    # print(len(x))\n",
    "    if len(x) > max:\n",
    "        max = len(x)\n",
    "        \n",
    "print(max)\n",
    "# train.iloc[9215].values\n",
    "\n",
    "epochs = 1\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to accomplish attention\n",
    "def batch_matmul_bias(seq, weight, bias):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s\n",
    "\n",
    "def batch_matmul(seq, weight):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hierarchical Attention Networks for Document Classification\n",
    "class AttentionWordRNN(nn.Module):\n",
    "    def __init__(self, batch_size, num_tokens, embed_size, word_gru_hidden, n_classes):\n",
    "        super(AttentionWordRNN, self).__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        self.embed_size = embed_size\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # (N, W) => (N, W, embed_size)\n",
    "        self.lookup = nn.Embedding(num_tokens, embed_size)\n",
    "        # (seq_len, batch, input_size) + (num_layers * num_directions, batch, hidden_size)\n",
    "        # => (seq_len, batch, hidden_size * num_directions) + (num_layers * num_directions, batch, hidden_size)\n",
    "        self.word_gru = nn.GRU(embed_size, word_gru_hidden, 2, bidirectional= True)\n",
    "        self.softmax_word = nn.Softmax(dim=1)\n",
    "        self.final_softmax = nn.LogSoftmax(dim=1)\n",
    "        # ???\n",
    "        self.final_linear = nn.Linear(2*word_gru_hidden, n_classes)\n",
    "        \n",
    "        self.weight_W_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 2*word_gru_hidden))\n",
    "        self.bias_word = nn.Parameter(torch.Tensor(2*word_gru_hidden,1))\n",
    "        self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n",
    "        \n",
    "        self.weight_W_word.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_word.data.uniform_(-0.1,0.1)\n",
    "    \n",
    "    def forward(self, x, state_word):\n",
    "        # embeddings\n",
    "        embedded = self.lookup(x)\n",
    "        embedded_resize = embedded.view(-1, self.batch_size, self.embed_size)\n",
    "        # word level gru\n",
    "        output_word, state_word = self.word_gru(embedded_resize, state_word)\n",
    "        # print(output_word.size())\n",
    "        word_hidden = batch_matmul_bias(output_word, self.weight_W_word, self.bias_word)\n",
    "        # print(word_hidden.size())\n",
    "        word_similarity = batch_matmul(word_hidden, self.weight_proj_word)\n",
    "        # print(word_similarity.size())\n",
    "        word_weights = self.softmax_word(word_similarity.transpose(1, 0))\n",
    "        # print(word_weights)\n",
    "        sen_vector = attention_mul(output_word, word_weights.transpose(1, 0))\n",
    "        # print(sen_vector)\n",
    "        final_linear = self.final_linear(sen_vector)\n",
    "        out = self.final_softmax(final_linear)\n",
    "        \n",
    "        return F.softmax(final_linear, dim=1), out, state_word\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(4, self.batch_size, self.word_gru_hidden)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "model = AttentionWordRNN(16, 47560, 256, 128, 3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    vx = Variable(torch.LongTensor(xtrain.astype(int))).cuda()\n",
    "    vy = Variable(torch.LongTensor(ytrain)).cuda()\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(0, len(xtrain), 16):\n",
    "        if i + 16 > len(xtrain):\n",
    "            vx_batch = vx[-17:-1]\n",
    "            vy_batch = vy[-17:-1]\n",
    "        else:\n",
    "            vx_batch = vx[i:i+16]\n",
    "            vy_batch = vy[i:i+16]\n",
    "            \n",
    "        hidden = model.init_hidden()\n",
    "        results, outputs, _ = model(vx_batch, hidden)\n",
    "        loss = criterion(outputs, vy_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, res = results.data.max(1)\n",
    "        print(\"after {}% training loss is {}\".format(float(i) / len(xtrain), loss.data[0]))\n",
    "        #print((torch.sum(res == vy_batch.data) + 0.0) / 16.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
