{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from cuda_functional import SRU, SRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = pd.read_json('imdb_final.json')\n",
    "# d['rating'] = d['rating'] - 1\n",
    "# d = d[['tokens','rating']]\n",
    "# X = d.tokens\n",
    "# y = d.rating\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.3, random_state= 42)\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "path = 'data_rct/'\n",
    "df_train_txt = pd.read_csv(path + 'training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "df_train_var = pd.read_csv(path + 'training_variants')\n",
    "df_test_txt = pd.read_csv(path + 'stage2_test_text.csv', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "df_test_var = pd.read_csv(path + 'stage2_test_variants.csv')\n",
    "df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\n",
    "df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')\n",
    "col = ['ID', 'Gene', 'Variation', 'Text', 'Class']\n",
    "df_train = df_train.loc[:, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "2   2     CBL                 Q249E   \n",
       "3   3     CBL                 N454D   \n",
       "4   4     CBL                 L399V   \n",
       "\n",
       "                                                Text  Class  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...      1  \n",
       "1   Abstract Background  Non-small cell lung canc...      2  \n",
       "2   Abstract Background  Non-small cell lung canc...      2  \n",
       "3  Recent evidence has demonstrated that acquired...      3  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CHEK2</td>\n",
       "      <td>H371Y</td>\n",
       "      <td>The incidence of breast cancer is increasing i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AXIN2</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>An unselected series of 310 colorectal carcino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>WNT4</td>\n",
       "      <td>E216G</td>\n",
       "      <td>Mycosis fungoides and Sézary syndrome are prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SUCLA2</td>\n",
       "      <td>G118R</td>\n",
       "      <td>Regulated progression through the cell cycle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>BRAF</td>\n",
       "      <td>T599insTT</td>\n",
       "      <td>Pilocytic astrocytoma (PA) is emerging as a tu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   1   CHEK2                 H371Y   \n",
       "1   2   AXIN2  Truncating Mutations   \n",
       "2   3    WNT4                 E216G   \n",
       "3   4  SUCLA2                 G118R   \n",
       "4   5    BRAF             T599insTT   \n",
       "\n",
       "                                                Text  \n",
       "0  The incidence of breast cancer is increasing i...  \n",
       "1  An unselected series of 310 colorectal carcino...  \n",
       "2  Mycosis fungoides and Sézary syndrome are prim...  \n",
       "3   Regulated progression through the cell cycle ...  \n",
       "4  Pilocytic astrocytoma (PA) is emerging as a tu...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4307"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[['Text']]\n",
    "X_test = df_test[['Text']]\n",
    "X_array = np.concatenate((X.values, X_test.values), axis=0)\n",
    "len(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481042"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts = Counter()\n",
    "for i in range(len(X_array)):\n",
    "    for sent in list(str(X_array[i][0]).split('.')):\n",
    "        for word in sent.split(' '):\n",
    "            total_counts[word] += 1\n",
    "\n",
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docX = []\n",
    "for i in range(len(X.values)):\n",
    "    sens = []\n",
    "    for sent in list(str(X.values[i][0]).split('.')):\n",
    "        sen = []\n",
    "        for word in sent.split(' '):\n",
    "            sen.append(word2index[word])\n",
    "        sens.append(sen)\n",
    "    docX.append(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docXt = []\n",
    "for i in range(len(X_test.values)):\n",
    "    sens = []\n",
    "    for sent in list(str(X_test.values[i][0]).split('.')):\n",
    "        sen = []\n",
    "        for word in sent.split(' '):\n",
    "            sen.append(word2index[word])\n",
    "        sens.append(sen)\n",
    "    docXt.append(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelY = df_train['Class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(docX), labelY, test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2656,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2656,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "#         print(seq[i].size())\n",
    "#         print(seq[i].size())\n",
    "#         print(weight.size())\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionWordRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, batch_size, num_tokens, embed_size, word_gru_hidden, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionWordRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.embed_size = embed_size\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lookup = nn.Embedding(num_tokens, embed_size)\n",
    "        if bidirectional == True:\n",
    "            self.word_gru = SRU(embed_size, word_gru_hidden, bidirectional= True)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,2*word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n",
    "        else:\n",
    "            self.word_gru = SRU(embed_size, word_gru_hidden, bidirectional= False)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(word_gru_hidden, word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(word_gru_hidden, 1))\n",
    "            \n",
    "        self.softmax_word = nn.Softmax()\n",
    "        self.weight_W_word.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_word.data.uniform_(-0.1,0.1)\n",
    "\n",
    "        \n",
    "    # 权重??? state_word???  \n",
    "    def forward(self, embed, state_word):\n",
    "        # embeddings\n",
    "        embedded = self.lookup(embed)\n",
    "        # word level gru\n",
    "#         print(embedded.size(), state_word.size())\n",
    "        output_word, state_word = self.word_gru(embedded, state_word)\n",
    "#         print output_word.size()\n",
    "        word_squish = batch_matmul_bias(output_word, self.weight_W_word,self.bias_word, nonlinearity='tanh')\n",
    "        word_attn = batch_matmul(word_squish, self.weight_proj_word)\n",
    "#         print(word_attn.size())\n",
    "#         print(word_attn.view(1, -1).transpose(1,0))\n",
    "        word_attn_norm = self.softmax_word(word_attn.transpose(1,0))\n",
    "        word_attn_vectors = attention_mul(output_word, word_attn_norm.transpose(1,0))        \n",
    "        return word_attn_vectors, state_word, word_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.word_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.word_gru_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionSentRNN(nn.Module):\n",
    "    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionSentRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sent_gru_hidden = sent_gru_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        \n",
    "        if bidirectional == True:\n",
    "            self.sent_gru = SRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden ,2* sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(2* sent_gru_hidden, n_classes)\n",
    "        else:\n",
    "            self.sent_gru = SRU(word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden ,sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(sent_gru_hidden, n_classes)\n",
    "        self.softmax_sent = nn.Softmax()\n",
    "        self.final_softmax = nn.Softmax()\n",
    "        self.weight_W_sent.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_sent.data.uniform_(-0.1,0.1)\n",
    "        \n",
    "        \n",
    "    def forward(self, word_attention_vectors, state_sent):\n",
    "#         print(word_attention_vectors.size(), state_sent.size())\n",
    "        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent)        \n",
    "        sent_squish = batch_matmul_bias(output_sent, self.weight_W_sent,self.bias_sent, nonlinearity='tanh')\n",
    "        sent_attn = batch_matmul(sent_squish, self.weight_proj_sent)\n",
    "        sent_attn_norm = self.softmax_sent(sent_attn.transpose(1,0))\n",
    "        sent_attn_vectors = attention_mul(output_sent, sent_attn_norm.transpose(1,0))        \n",
    "        # final classifier\n",
    "        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\n",
    "        return F.log_softmax(final_map), state_sent, sent_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "def pad_batch(mini_batch):\n",
    "    mini_batch_size = len(mini_batch)\n",
    "#     print(mini_batch)\n",
    "    max_sent_len = int(np.max([len(x) for x in mini_batch]))\n",
    "    max_token_len = int(np.max([len(val) for sublist in mini_batch for val in sublist]))\n",
    "    main_matrix = np.zeros((mini_batch_size, max_sent_len, max_token_len), dtype= np.int)\n",
    "    for i in range(main_matrix.shape[0]):\n",
    "        for j in range(main_matrix.shape[1]):\n",
    "            for k in range(main_matrix.shape[2]):\n",
    "                try:\n",
    "                    main_matrix[i,j,k] = mini_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    # sen_len * batch * word_vec\n",
    "    return Variable(torch.from_numpy(main_matrix).transpose(0,1))\n",
    "    \n",
    "    \n",
    "def gen_minibatch(tokens, labels, mini_batch_size, shuffle=False):\n",
    "    for token, label in iterate_minibatches(tokens, labels, mini_batch_size, shuffle):\n",
    "        token = pad_batch(token)\n",
    "        yield token.cuda(), Variable(torch.from_numpy(label), requires_grad= False).cuda()  \n",
    "#         yield token, Variable(torch.from_numpy(label), requires_grad= False) \n",
    "        \n",
    "\n",
    "def get_predictions(val_tokens, word_attn_model, sent_attn_model):\n",
    "    max_sents, batch_size, max_tokens = val_tokens.size()\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "#     state_word = word_attn_model.init_hidden()\n",
    "    \n",
    "    state_sent = sent_attn_model.init_hidden().cuda()  \n",
    "#     state_sent = sent_attn_model.init_hidden()  \n",
    "    \n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(val_tokens[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)    \n",
    "    return y_pred\n",
    "\n",
    "def test_accuracy_mini_batch(tokens, labels, word_attn, sent_attn):\n",
    "    y_pred = get_predictions(tokens, word_attn, sent_attn)\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    correct = np.ndarray.flatten(y_pred.data.cpu().numpy())\n",
    "    labels = np.ndarray.flatten(labels.data.cpu().numpy())\n",
    "    num_correct = sum(correct == labels)\n",
    "    return float(num_correct) / len(correct)\n",
    "        \n",
    "def test_accuracy_full_batch(tokens, labels, mini_batch_size, word_attn, sent_attn):\n",
    "    p = []\n",
    "    l = []\n",
    "    g = gen_minibatch(tokens, labels, mini_batch_size)\n",
    "    for token, label in g:\n",
    "        y_pred = get_predictions(token.cuda(), word_attn, sent_attn)\n",
    "#         y_pred = get_predictions(token, word_attn, sent_attn)\n",
    "        \n",
    "        _, y_pred = torch.max(y_pred, 1)\n",
    "        p.append(np.ndarray.flatten(y_pred.data.cpu().numpy()))\n",
    "        l.append(np.ndarray.flatten(label.data.cpu().numpy()))\n",
    "    p = [item for sublist in p for item in sublist]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    p = np.array(p)\n",
    "    l = np.array(l)\n",
    "    num_correct = sum(p == l)\n",
    "    return float(num_correct)/ len(p)\n",
    "    \n",
    "def check_val_loss(val_tokens, val_labels, mini_batch_size, word_attn_model, sent_attn_model):\n",
    "    val_loss = []\n",
    "    for token, label in iterate_minibatches(val_tokens, val_labels, mini_batch_size, shuffle=False):\n",
    "        val_loss.append(test_data(pad_batch(token).cuda(), Variable(torch.from_numpy(label), requires_grad= False).cuda(), \n",
    "                                  word_attn_model, sent_attn_model))\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data(mini_batch, targets, word_attn_model, sent_attn_model, word_optimizer, sent_optimizer, criterion):\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "#     state_word = word_attn_model.init_hidden()\n",
    "    \n",
    "#     \n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "#     state_sent = sent_attn_model.init_hidden()\n",
    "    \n",
    "    \n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    word_optimizer.zero_grad()\n",
    "    sent_optimizer.zero_grad()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        _s = _s.unsqueeze(0)\n",
    "#         print(_s.size())\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "            \n",
    "    print(s.size())\n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(y_pred.cuda(), targets) \n",
    "#     loss = criterion(y_pred, targets) \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    word_optimizer.step()\n",
    "    sent_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_attn = AttentionWordRNN(\n",
    "    batch_size=2, \n",
    "    num_tokens=481042, \n",
    "    embed_size=256,                    \n",
    "    word_gru_hidden=64, \n",
    "    bidirectional= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_attn = AttentionSentRNN(\n",
    "    batch_size=2, \n",
    "    sent_gru_hidden=64, \n",
    "    word_gru_hidden=64,             \n",
    "    n_classes=9, \n",
    "    bidirectional= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "word_optmizer = optim.Adam(word_attn.parameters(), lr=learning_rate)\n",
    "sent_optimizer = optim.Adam(sent_attn.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionWordRNN (\n",
       "  (lookup): Embedding(481042, 256)\n",
       "  (word_gru): SRU (\n",
       "    (rnn_lst): ModuleList (\n",
       "      (0): SRUCell (\n",
       "      )\n",
       "      (1): SRUCell (\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (softmax_word): Softmax ()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionSentRNN (\n",
       "  (sent_gru): SRU (\n",
       "    (rnn_lst): ModuleList (\n",
       "      (0): SRUCell (\n",
       "      )\n",
       "      (1): SRUCell (\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_linear): Linear (128 -> 9)\n",
       "  (softmax_sent): Softmax ()\n",
       "  (final_softmax): Softmax ()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_early_stopping(\n",
    "    mini_batch_size,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    word_attn_model, sent_attn_model,\n",
    "    word_attn_optimizer, sent_attn_optimizer,\n",
    "    loss_criterion, num_epoch,\n",
    "    print_val_loss_every = 1000,\n",
    "    print_loss_every = 50\n",
    "):\n",
    "    start = time.time()\n",
    "    loss_full = []\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    loss_smooth = []\n",
    "    accuracy_full = []\n",
    "    epoch_counter = 0\n",
    "    g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "    \n",
    "    for i in range(1, num_epoch + 1):\n",
    "        try:\n",
    "            tokens, labels = next(g)\n",
    "            loss = train_data(tokens, labels, word_attn_model, sent_attn_model, word_attn_optimizer, sent_attn_optimizer, loss_criterion)\n",
    "            acc = test_accuracy_mini_batch(tokens, labels, word_attn_model, sent_attn_model)\n",
    "            accuracy_full.append(acc)\n",
    "            accuracy_epoch.append(acc)\n",
    "            loss_full.append(loss)\n",
    "            loss_epoch.append(loss)\n",
    "            # print loss every n passes\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss at %d minibatches, %d epoch,(%s) is %f' %(i, epoch_counter, timeSince(start, i / num_epoch), np.mean(loss_epoch)))\n",
    "                print('Accuracy at %d minibatches is %f' % (i, np.mean(accuracy_epoch)))\n",
    "            # check validation loss every n passes\n",
    "            if i % print_val_loss_every == 0:\n",
    "                val_loss = check_val_loss(X_test, y_test, mini_batch_size, word_attn_model, sent_attn_model)\n",
    "                print('Average training loss at this epoch..minibatch..%d..is %f' % (i, np.mean(loss_epoch)))\n",
    "                print('Validation loss after %d passes is %f' %(i, val_loss))\n",
    "                if val_loss > np.mean(loss_full):\n",
    "                    print('Validation loss is higher than training loss at %d is %f , stopping training!' % (i, val_loss))\n",
    "                    print('Average training loss at %d is %f' % (i, np.mean(loss_full)))\n",
    "            \n",
    "        except StopIteration:\n",
    "            epoch_counter += 1\n",
    "            print('Reached %d epocs' % epoch_counter)\n",
    "            print('i %d' % i)\n",
    "            g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "            loss_epoch = []\n",
    "            accuracy_epoch = []\n",
    "        \n",
    "    return loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([230, 2, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4c9236bb6d18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_full= train_early_stopping(2, X_train, y_train, X_test, y_test, word_attn, sent_attn, word_optmizer, sent_optimizer, \n\u001b[0;32m----> 2\u001b[0;31m                             criterion, 5, 5, 1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-daf52c292151>\u001b[0m in \u001b[0;36mtrain_early_stopping\u001b[0;34m(mini_batch_size, X_train, y_train, X_test, y_test, word_attn_model, sent_attn_model, word_attn_optimizer, sent_attn_optimizer, loss_criterion, num_epoch, print_val_loss_every, print_loss_every)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_attn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_attn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_attn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_attn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_accuracy_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_attn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_attn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maccuracy_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ea8f9e5193f9>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(mini_batch, targets, word_attn_model, sent_attn_model, word_optimizer, sent_optimizer, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#     loss = criterion(y_pred, targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mword_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    192\u001b[0m         tensor_args = [arg.data if isinstance(arg, Variable) else arg\n\u001b[1;32m    193\u001b[0m                        for arg in args]\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;31m# XXX: this is only an approximation of these flags - there's no way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# to figure out if fn didn't use ctx.saved_variables and as a result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quoniammm/anaconda3/envs/py3Tfgpu/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0m_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Doesn't support Variable grad_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             ctx._backend.LookupTable_accGradParameters(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "loss_full= train_early_stopping(2, X_train, y_train, X_test, y_test, word_attn, sent_attn, word_optmizer, sent_optimizer, \n",
    "                            criterion, 5, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
