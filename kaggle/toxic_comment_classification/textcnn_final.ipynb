{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quoniammm/anaconda3/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, gc, numpy as np, \\\n",
    "pandas as pd, pickle as pkl, tensorflow as tf\n",
    "\n",
    "#=================Keras==============\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Conv1D, Conv2D, \\\n",
    "Embedding, Dropout, Activation, Permute\n",
    "from keras.layers import Bidirectional, MaxPooling1D, MaxPooling2D, \\\n",
    "Reshape, Flatten, concatenate, BatchNormalization, GlobalMaxPool1D, \\\n",
    "GlobalMaxPool2D\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, backend\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Don't Show Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#=================nltk===============\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#=================gensim=============\n",
    "import gensim\n",
    "#=================save_list==========\n",
    "import pickle\n",
    "#=================sklearn============\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.save_word2vec_format('./word2vec/GoogleNews-vectors-negative300.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "comp = ''\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "number_filters = 100 # the number of CNN filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "train = train.reindex(np.random.permutation(train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess or not \n",
    "preprocess = False\n",
    "\n",
    "if preprocess:\n",
    "    comments = []\n",
    "    for text in list_sentences_train:\n",
    "        comments.append(text_to_wordlist(text))\n",
    "    \n",
    "    test_comments=[]\n",
    "    for text in list_sentences_test:\n",
    "        test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "else:\n",
    "    comments = list_sentences_train\n",
    "    test_comments = list_sentences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenlize\n",
    "if preprocess:\n",
    "    tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "else:\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list(comments) + list(test_comments)))\n",
    "comments_sequence = tokenizer.texts_to_sequences(comments)\n",
    "test_comments_sequence = tokenizer.texts_to_sequences(test_comments)    \n",
    "X_t = pad_sequences(comments_sequence , maxlen=maxlen)\n",
    "X_te = pad_sequences(test_comments_sequence, maxlen=maxlen)\n",
    "\n",
    "EMBEDDING_FILE=f'{path}glove6b/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "with open(\"emb_mean.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(emb_mean, fp)\n",
    "    \n",
    "with open(\"emb_mean.txt\", \"rb\") as fp:   # Unpickling\n",
    "    emb_mean = pickle.load(fp)\n",
    "    \n",
    "with open(\"emb_std.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(emb_std, fp)\n",
    "    \n",
    "with open(\"emb_std.txt\", \"rb\") as fp:   # Unpickling\n",
    "    emb_std = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_t.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(X_t, fp)\n",
    "    \n",
    "with open(\"X_te.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(X_te, fp)\n",
    "    \n",
    "with open(\"embed_glove.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(embedding_matrix, fp)\n",
    "    \n",
    "with open(\"embed_word2vec.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(embedding_matrix, fp)\n",
    "    \n",
    "with open(\"X_t.txt\", \"rb\") as fp:   # Unpickling\n",
    "    X_t = pickle.load(fp)\n",
    "    \n",
    "with open(\"X_te.txt\", \"rb\") as fp:   # Unpickling\n",
    "    X_te = pickle.load(fp)\n",
    "    \n",
    "with open(\"embed_glove.txt\", \"rb\") as fp:   # Unpickling\n",
    "    embedding_matrix_glove = pickle.load(fp)\n",
    "\n",
    "# with open(\"embed_word2vec.txt\", \"rb\") as fp:   # Unpickling\n",
    "#     embedding_matrix_word2vec = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train ,y_eval = train_test_split(X_t, y,test_size=0.25,shuffle=True,\n",
    "                                                    random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 50, 1)\n",
      "(?, 100, 50, 2)\n",
      "(?, 300)\n",
      "(?, 6)\n"
     ]
    }
   ],
   "source": [
    "# filter_size\n",
    "filter_size = [3, 4, 5]\n",
    "\n",
    "inp = Input(shape=(maxlen, ))\n",
    "x1 = Embedding(max_features, embed_size, weights=[embedding_matrix_glove], trainable=True)(inp)\n",
    "x2 = Embedding(max_features, embed_size, weights=[embedding_matrix_glove], trainable=False)(inp)\n",
    "# x3 = Embedding(max_features, embed_size)(inp)\n",
    "x1 = Reshape((100, 50, 1))(x1)\n",
    "x2 = Reshape((100, 50, 1))(x2)\n",
    "# x3 = Reshape((100, 50, 1))(x3)\n",
    "print(x1.shape)\n",
    "x = concatenate([x1, x2])\n",
    "print(x.shape)\n",
    "\n",
    "# Version of Conv1D\n",
    "# for fz in filter_size:\n",
    "# conv_blocks = []\n",
    "# for sz in filter_size:\n",
    "#     conv = Conv1D(number_filters, sz)(x)\n",
    "#     batch_norm = BatchNormalization()(conv)\n",
    "#     activation = Activation('elu')(batch_norm)\n",
    "#     print(activation.shape)\n",
    "#     pooling = GlobalMaxPool1D()(activation)\n",
    "#     conv_blocks.append(pooling)\n",
    "\n",
    "# Version of Conv2D\n",
    "conv_blocks = []\n",
    "for sz in filter_size:\n",
    "    conv = Conv2D(number_filters, (sz, embed_size), data_format='channels_last')(x)\n",
    "    batch_norm = BatchNormalization()(conv)\n",
    "    activation = Activation('elu')(batch_norm)\n",
    "    pooling = GlobalMaxPool2D()(activation)\n",
    "    conv_blocks.append(pooling)\n",
    "    \n",
    "x = concatenate(conv_blocks)\n",
    "print(x.shape)\n",
    "# x = Dense(128, activation=\"relu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "print(x.shape)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "save_best = ModelCheckpoint(\n",
    "    'toxic.hdf', \n",
    "    save_best_only=True,                     \n",
    "    monitor='val_loss', \n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/30\n",
      "119678/119678 [==============================] - 54s 454us/step - loss: 0.0639 - acc: 0.9783 - val_loss: 0.0530 - val_acc: 0.9810\n",
      "Epoch 2/30\n",
      "119678/119678 [==============================] - 48s 400us/step - loss: 0.0473 - acc: 0.9827 - val_loss: 0.0481 - val_acc: 0.9824\n",
      "Epoch 3/30\n",
      "119678/119678 [==============================] - 48s 402us/step - loss: 0.0413 - acc: 0.9844 - val_loss: 0.0526 - val_acc: 0.9809\n",
      "Epoch 4/30\n",
      "119678/119678 [==============================] - 50s 420us/step - loss: 0.0361 - acc: 0.9861 - val_loss: 0.0492 - val_acc: 0.9824\n",
      "Epoch 5/30\n",
      "119678/119678 [==============================] - 50s 420us/step - loss: 0.0315 - acc: 0.9877 - val_loss: 0.0538 - val_acc: 0.9809\n",
      "Epoch 6/30\n",
      "119678/119678 [==============================] - 51s 423us/step - loss: 0.0264 - acc: 0.9898 - val_loss: 0.0549 - val_acc: 0.9815\n",
      "Epoch 7/30\n",
      "119678/119678 [==============================] - 51s 423us/step - loss: 0.0216 - acc: 0.9918 - val_loss: 0.0612 - val_acc: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f106e1c2ef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, validation_data=(X_eval, y_eval),\n",
    "    epochs=30, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping,save_best]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.load_weights(filepath = 'toxic.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 9s 59us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=256, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission_textcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
