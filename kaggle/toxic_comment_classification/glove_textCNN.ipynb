{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "#=================Keras==============\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Conv2D, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, MaxPooling1D, MaxPooling2D, Reshape, Flatten, concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, backend\n",
    "#=================nltk===============\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "comp = ''\n",
    "EMBEDDING_FILE=f'{path}glove6b/glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "number_filters = 100 # the number of CNN filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "# tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(comments + test_comments))\n",
    "comments_sequence = tokenizer.texts_to_sequences(comments)\n",
    "test_comments_sequence = tokenizer.texts_to_sequences(test_comments)    \n",
    "X_t = pad_sequences(comments_sequence , maxlen=maxlen)\n",
    "X_te = pad_sequences(test_comments_sequence, maxlen=maxlen)\n",
    "\n",
    "X_t = X_t.reshape((X_t.shape[0], 1, X_t.shape[1]))\n",
    "X_te = X_te.reshape((X_te.shape[0], 1, X_te.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(1, maxlen,))\n",
    "# x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "# x = concatenate([x_1, x_2])\n",
    "\n",
    "x1 = Conv2D(number_filters, (3, embed_size), data_format='channels_first')(x)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "x1 = MaxPooling2D((int(int(x1.shape[2])  / 1.5), 1), data_format='channels_first')(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "x2 = Conv2D(number_filters, (1, embed_size), data_format='channels_first')(x)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('elu')(x2)\n",
    "x2 = MaxPooling2D((int(int(x2.shape[2])  / 1.5), 1), data_format='channels_first')(x2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "x3 = Conv2D(number_filters, (2, embed_size), data_format='channels_first')(x)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = MaxPooling2D((int(int(x3.shape[2])  / 1.5), 1), data_format='channels_first')(x3)\n",
    "x3 = Flatten()(x3)\n",
    "\n",
    "x4 = Conv2D(number_filters, (3, embed_size), data_format='channels_first')(x)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('elu')(x4)\n",
    "x4 = MaxPooling2D((int(int(x4.shape[2])  / 1.5), 1), data_format='channels_first')(x4)\n",
    "x4 = Flatten()(x4)\n",
    "\n",
    "x5 = Conv2D(number_filters, (4, embed_size), data_format='channels_first')(x)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Activation('relu')(x5)\n",
    "x5 = MaxPooling2D((int(int(x5.shape[2])  / 1.5), 1), data_format='channels_first')(x5)\n",
    "x5 = Flatten()(x5)\n",
    "\n",
    "x6 = Conv2D(number_filters, (5, embed_size), data_format='channels_first')(x)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Activation('elu')(x6)\n",
    "x6 = MaxPooling2D((int(int(x6.shape[2])  / 1.5), 1), data_format='channels_first')(x6)\n",
    "x6 = Flatten()(x6)\n",
    "\n",
    "x7 = Conv2D(number_filters, (6, embed_size), data_format='channels_first')(x)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = Activation('relu')(x7)\n",
    "x7 = MaxPooling2D((int(int(x7.shape[2])  / 1.5), 1), data_format='channels_first')(x7)\n",
    "x7 = Flatten()(x7)\n",
    "\n",
    "x8 = Conv2D(number_filters, (7, embed_size), data_format='channels_first')(x)\n",
    "x8 = BatchNormalization()(x8)\n",
    "x8 = Activation('elu')(x8)\n",
    "x8 = MaxPooling2D((int(int(x8.shape[2])  / 1.5), 1), data_format='channels_first')(x8)\n",
    "x8 = Flatten()(x8)\n",
    "\n",
    "x9 = Conv2D(number_filters, (8, embed_size), data_format='channels_first')(x)\n",
    "x9 = BatchNormalization()(x9)\n",
    "x9 = Activation('relu')(x9)\n",
    "x9 = MaxPooling2D((int(int(x9.shape[2])  / 1.5), 1), data_format='channels_first')(x9)\n",
    "x9 = Flatten()(x9)\n",
    "\n",
    "x10 = Conv2D(number_filters, (9, embed_size), data_format='channels_first')(x)\n",
    "x10 = BatchNormalization()(x10)\n",
    "x10 = Activation('elu')(x10)\n",
    "x10 = MaxPooling2D((int(int(x10.shape[2])  / 1.5), 1), data_format='channels_first')(x10)\n",
    "x10 = Flatten()(x10)\n",
    "\n",
    "x = concatenate([x1, x2, x3, x4, x5, x6, x7, x8, x9, x10])\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(512, activation=\"elu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(256, activation=\"relu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(6, activation=\"softmax\")(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_t, y, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 10s 42us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission_textcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as far as nicknames go this article is embarrassing, Where is the Human fish, golden fish, flying fish, and the American super fish, among others? wiki should be ashamed for this mess of an article. ~Anonymous *and there I signed the post*'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train[\"comment_text\"].values[8]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as far as nicknames go this article is embarrassing Where is the Human fish golden fish flying fish and the American super fish among others wiki should be ashamed for this mess of an article Anonymous and there I signed the post'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = special_character_removal.sub('', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as far as nicknames go this article is embarrassing Where is the Human fish golden fish flying fish and the American super fish among others wiki should be ashamed for this mess of an article Anonymous and there I signed the post'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = replace_numbers.sub('n', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'far nicknames go article embarrassing human fish golden fish flying fish american super fish among others wiki ashamed mess article anonymous signed post'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower().split()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "text = [w for w in text if not w in stops]\n",
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'far nicknam go articl embarrass human fish golden fish fli fish american super fish among other wiki asham mess articl anonym sign post'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.split()\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [stemmer.stem(word) for word in text]\n",
    "text = \" \".join(stemmed_words)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
